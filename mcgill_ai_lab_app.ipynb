{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "232668ab",
      "metadata": {
        "id": "232668ab"
      },
      "source": [
        "# **News Bias Analysis For AI Lab Application**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84be712",
      "metadata": {
        "id": "f84be712"
      },
      "source": [
        "1. **Summary of the [paper](https://arxiv.org/abs/1607.06520)**\n",
        "\n",
        "> The paper covers gender stereotypes in word embeddings and explores methods of limiting bias or completely debiasing these word embeddings while still maintaining embedding utility.\n",
        " Firstly, the authors explain how studying stereotypes results in more consistency as opposed to studying bias. The different types of bias are covered, establishing the distinction between direct bias and indirect bias. Direct bias is explained as being when gender-neutral words are closer to one gender over the other. For example, “nurse” is associated with “she” much more often than “he”. Indirect bias is when relationships between gender-neutral words reflect gender associations. For example, the fact that the words “bookkeeper” and “receptionist” are much closer to “softball” than “football” may be due to female associations with “bookkeeper”, “receptionist” and “softball”.\n",
        "To address these biases, two approaches to debiasing word embeddings were introduced. Before these approaches however, the gender subspace was introduced. The gender subspace is the gender direction computed by taking the average difference across gender pairs. This establishes the area of our total vector space that concerns gender.\n",
        "The first approach is called “hard debiasing”, where we remove gender bias completely. First for gender-neutral pairs, we begin by projecting the word vector off the gender direction so its gender component is zero. Then for explicit gender pairs, we make sure they are exactly equal distance from any gender-neutral word but we keep their distinction if they’re used in other contexts (for example, we use the phrase “grandfather a regulation” instead of “grandmother a regulation”) by maintaining parts of the vector outside the gender direction.\n",
        "The second approach is called “soft debiasing\", where we reduce bias by keeping some of the embedding’s original structure. First we apply a linear transformation to the embedding and try to keep word meanings (dot products) the same, while pushing gender-neutral words away from the gender direction. We can use parameter λ (lambda) to control how aggressive the debiasing is. If λ is large, then the debiasing is more aggressive and similar to hard debiasing. But if λ is smaller, then the debiasing preserves more of the structure.\n",
        "These approaches of debiasing are evaluated and it is found that debiasing does not degrade performance. It was also found that hard debiasing did indeed decrease bias (from 19% to 6%).\n",
        "These methods would definitely apply to our media bias project, as we can identify a “bias subspace” (for example, left–right, Israel–Palestine, Democrat–Republican), which would allow us to isolate and compare portrayals across ideological lines in word embedding spaces. Just like gender-neutral words like “nurse” were projected along the gender axis in the paper, we could project political terms (for example, “leader”, “attack”, “peace”) along a left-right axis to examine directions. After doing this we might consider adapting the debiasing approach discussed in the paper to measure or neutralize political slant in certain applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d592a230",
      "metadata": {
        "id": "d592a230"
      },
      "source": [
        "2. **Analyze analogy generation**\n",
        "\n",
        "\n",
        "> The paper generates analogies using the formula:  \n",
        ">  \n",
        "> $$ S(a, b, x, y) = \\cos(a - b,\\; x - y) $$\n",
        "> where **a**, **b**, **x**, and **y** are word vectors. This measures how parallel the vector difference $a - b$ is to $x - y$ (how similar their directional relationships are)  \n",
        ">  \n",
        "> In the paper’s example:  $a = \\text{\"she\"}$  and $b = \\text{\"he\"}$  \n",
        ">  \n",
        "> They then generate $(x, y)$ pairs such that: $a - b \\approx x - y$\n",
        ">\n",
        ">  \n",
        "> which lets them find analogies similar to \"she is to he as x is to y\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "from itertools import combinations\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JKh3DYw7MQE",
        "outputId": "bb7f0f32-6169-4d75-bbdf-a18e387ef246"
      },
      "id": "3JKh3DYw7MQE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "7tfeJbon-vf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325aedec-b29d-4f5d-9984-9fb472b4460f"
      },
      "id": "7tfeJbon-vf3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conflict_words = [\n",
        "    'soldier', 'militant', 'terrorist', 'rebel', 'army', 'government', 'resistance', 'occupation',\n",
        "    'leader', 'official', 'violence', 'peace', 'attack', 'defense', 'strike', 'conflict',\n",
        "    'freedom', 'hostage', 'war', 'ceasefire', 'minister', 'state', 'nation', 'military', 'protest',\n",
        "    'uprising', 'regime', 'diplomat', 'oppression', 'autonomy', 'settlement'\n",
        "]\n",
        "valid_words = [w for w in conflict_words if w in model]"
      ],
      "metadata": {
        "id": "eIryG5FD-3Ir"
      },
      "id": "eIryG5FD-3Ir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6c0c85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce6c0c85",
        "outputId": "3ac0ecba-90ac-4981-9f84-a70ce8bf2c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top analogies aligned with israel → palestine:\n",
            "\n",
            "occupation   → oppression    (similarity: 0.0444)\n",
            "militant     → rebel         (similarity: 0.0310)\n",
            "occupation   → war           (similarity: 0.0135)\n",
            "militant     → terrorist     (similarity: -0.0035)\n",
            "army         → military      (similarity: -0.0142)\n",
            "conflict     → war           (similarity: -0.0715)\n",
            "peace        → ceasefire     (similarity: -0.1128)\n"
          ]
        }
      ],
      "source": [
        "a, b = model['israel'], model['palestine']\n",
        "bias_direction = b - a\n",
        "\n",
        "\n",
        "threshold = 0.5\n",
        "analogies = []\n",
        "\n",
        "for x, y in combinations(valid_words, 2):\n",
        "    vec_x, vec_y = model[x], model[y]\n",
        "    similarity = np.dot(vec_x, vec_y) / (np.linalg.norm(vec_x) * np.linalg.norm(vec_y))  # cosine similarity\n",
        "\n",
        "    if similarity >= threshold: # reasonable pairs\n",
        "        diff = vec_y - vec_x\n",
        "        sim = np.dot(bias_direction, diff) / (np.linalg.norm(bias_direction) * np.linalg.norm(diff)) # similarity between diff and bias direction\n",
        "        analogies.append((x, y, sim))\n",
        "\n",
        "analogies.sort(key=lambda tup: -tup[2]) # sort descending based on similarity score\n",
        "top_pairs = analogies[:15] # top 15 pairs most similar to israel -> palestine\n",
        "\n",
        "print(\"top analogies aligned with israel → palestine:\\n\")\n",
        "for x, y, score in top_pairs:\n",
        "    print(f\"{x:12} → {y:12}  (similarity: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7dc403",
      "metadata": {
        "id": "6e7dc403"
      },
      "source": [
        "3. **Compare portrayals**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# good-bad portrayal axis\n",
        "\n",
        "good_words = ['good', 'peaceful', 'honest', 'hero', 'liberator']\n",
        "bad_words = ['bad', 'violent', 'corrupt', 'terrorist', 'oppressor']\n",
        "\n",
        "# creating a vector (portrayal axis) by averaging the direction from each good word to its bad counterpart\n",
        "portrayal_axis = np.mean([model[bad] - model[good] for good, bad in zip(good_words, bad_words)], axis=0)\n"
      ],
      "metadata": {
        "id": "D3uFdkpZ_qjF"
      },
      "id": "D3uFdkpZ_qjF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing directional bias across each pair\n",
        "\n",
        "print(\"portrayal bias (positive = more aligned with 'bad'):\\n\")\n",
        "for x, y, _ in top_pairs:\n",
        "    # projections onto portrayal axis to see which side the words are more aligned with\n",
        "    proj_x = np.dot(model[x], portrayal_axis) / np.linalg.norm(portrayal_axis)\n",
        "    proj_y = np.dot(model[y], portrayal_axis) / np.linalg.norm(portrayal_axis)\n",
        "    print(f\"{x:12} score: {proj_x:+.4f}   |   {y:12} score: {proj_y:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m3kYpIoALwb",
        "outputId": "c245a7c1-3c76-46da-8243-f55bbb7be695"
      },
      "id": "_m3kYpIoALwb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "portrayal bias (positive = more aligned with 'bad'):\n",
            "\n",
            "occupation   score: +0.3438   |   oppression   score: +0.4714\n",
            "militant     score: +1.1886   |   rebel        score: +0.5857\n",
            "occupation   score: +0.3438   |   war          score: +0.1753\n",
            "militant     score: +1.1886   |   terrorist    score: +1.8119\n",
            "army         score: +0.0363   |   military     score: +0.2891\n",
            "conflict     score: +0.2326   |   war          score: +0.1753\n",
            "peace        score: -0.3424   |   ceasefire    score: +0.0579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of the Portrayals**\n",
        "\n",
        "I took the top analogy pairs that were most similar to the direction israel → palestine, and then projected both words in each pair onto a “good–bad” axis made from opposite pairs like peaceful → violent and hero → terrorist.\n",
        "\n",
        "The results show that the model tends to frame words like \"ceasefire\" less positively than terms like \"peace\". It reflects how even synonyms or related terms can be portrayed very differently depending on context and media framing."
      ],
      "metadata": {
        "id": "wq6nLhqJAsja"
      },
      "id": "wq6nLhqJAsja"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}