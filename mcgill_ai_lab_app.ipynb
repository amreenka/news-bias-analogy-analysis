{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "232668ab",
      "metadata": {
        "id": "232668ab"
      },
      "source": [
        "# **News Bias Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84be712",
      "metadata": {
        "id": "f84be712"
      },
      "source": [
        "1. **Summary of the [paper](https://arxiv.org/abs/1607.06520)**\n",
        "\n",
        "> The paper covers gender stereotypes in word embeddings and explores methods of limiting bias or completely debiasing these word embeddings while still maintaining embedding utility.\n",
        " Firstly, the authors explain how studying stereotypes results in more consistency as opposed to studying bias. The different types of bias are covered, establishing the distinction between direct bias and indirect bias. Direct bias is explained as being when gender-neutral words are closer to one gender over the other. For example, “nurse” is associated with “she” much more often than “he”. Indirect bias is when relationships between gender-neutral words reflect gender associations. For example, the fact that the words “bookkeeper” and “receptionist” are much closer to “softball” than “football” may be due to female associations with “bookkeeper”, “receptionist” and “softball”.\n",
        "To address these biases, two approaches to debiasing word embeddings were introduced. Before these approaches however, the gender subspace was introduced. The gender subspace is the gender direction computed by taking the average difference across gender pairs. This establishes the area of our total vector space that concerns gender.\n",
        "The first approach is called “hard debiasing”, where we remove gender bias completely. First for gender-neutral pairs, we begin by projecting the word vector off the gender direction so its gender component is zero. Then for explicit gender pairs, we make sure they are exactly equal distance from any gender-neutral word but we keep their distinction if they’re used in other contexts (for example, we use the phrase “grandfather a regulation” instead of “grandmother a regulation”) by maintaining parts of the vector outside the gender direction.\n",
        "The second approach is called “soft debiasing\", where we reduce bias by keeping some of the embedding’s original structure. First we apply a linear transformation to the embedding and try to keep word meanings (dot products) the same, while pushing gender-neutral words away from the gender direction. We can use parameter λ (lambda) to control how aggressive the debiasing is. If λ is large, then the debiasing is more aggressive and similar to hard debiasing. But if λ is smaller, then the debiasing preserves more of the structure.\n",
        "These approaches of debiasing are evaluated and it is found that debiasing does not degrade performance. It was also found that hard debiasing did indeed decrease bias (from 19% to 6%).\n",
        "These methods would definitely apply to our media bias project, as we can identify a “bias subspace” (for example, left–right, Israel–Palestine, Democrat–Republican), which would allow us to isolate and compare portrayals across ideological lines in word embedding spaces. Just like gender-neutral words like “nurse” were projected along the gender axis in the paper, we could project political terms (for example, “leader”, “attack”, “peace”) along a left-right axis to examine directions. After doing this we might consider adapting the debiasing approach discussed in the paper to measure or neutralize political slant in certain applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d592a230",
      "metadata": {
        "id": "d592a230"
      },
      "source": [
        "2. **Analyze analogy generation**\n",
        "\n",
        "\n",
        "> The paper generates analogies using the formula:  \n",
        ">  \n",
        "> $$ S(a, b, x, y) = \\cos(a - b,\\; x - y) $$\n",
        "> where **a**, **b**, **x**, and **y** are word vectors. This measures how parallel the vector difference $a - b$ is to $x - y$ (how similar their directional relationships are)  \n",
        ">  \n",
        "> In the paper’s example:  $a = \\text{\"she\"}$  and $b = \\text{\"he\"}$  \n",
        ">  \n",
        "> They then generate $(x, y)$ pairs such that: $a - b \\approx x - y$\n",
        ">\n",
        ">  \n",
        "> which lets them find analogies similar to \"she is to he as x is to y\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "from itertools import combinations\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JKh3DYw7MQE",
        "outputId": "bf1c09c0-c067-4276-96b7-1762af346fe6"
      },
      "id": "3JKh3DYw7MQE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "7tfeJbon-vf3"
      },
      "id": "7tfeJbon-vf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conflict_words = [\n",
        "    'soldier', 'militant', 'terrorist', 'rebel', 'army', 'government', 'resistance', 'occupation',\n",
        "    'leader', 'official', 'violence', 'peace', 'attack', 'defense', 'strike', 'conflict',\n",
        "    'freedom', 'hostage', 'war', 'ceasefire', 'minister', 'state', 'nation', 'military', 'protest',\n",
        "    'uprising', 'regime', 'diplomat', 'oppression', 'autonomy', 'settlement'\n",
        "]\n",
        "valid_words = [w for w in conflict_words if w in model]"
      ],
      "metadata": {
        "id": "eIryG5FD-3Ir"
      },
      "id": "eIryG5FD-3Ir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6c0c85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce6c0c85",
        "outputId": "8fc2593e-b56f-4955-b1a9-3c2c60a7621a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top analogies aligned with israel → palestine:\n",
            "\n",
            "ceasefire    → uprising      (similarity: 0.2169)\n",
            "occupation   → uprising      (similarity: 0.2074)\n",
            "army         → uprising      (similarity: 0.2027)\n",
            "military     → uprising      (similarity: 0.1987)\n",
            "war          → uprising      (similarity: 0.1944)\n",
            "defense      → uprising      (similarity: 0.1603)\n",
            "strike       → uprising      (similarity: 0.1588)\n",
            "peace        → uprising      (similarity: 0.1520)\n",
            "government   → uprising      (similarity: 0.1485)\n",
            "militant     → uprising      (similarity: 0.1465)\n",
            "attack       → uprising      (similarity: 0.1427)\n",
            "conflict     → uprising      (similarity: 0.1423)\n",
            "nation       → uprising      (similarity: 0.1375)\n",
            "army         → leader        (similarity: 0.1374)\n",
            "freedom      → uprising      (similarity: 0.1371)\n"
          ]
        }
      ],
      "source": [
        "a, b = model['israel'], model['palestine']\n",
        "bias_direction = b - a\n",
        "\n",
        "analogies = []\n",
        "\n",
        "for x, y in combinations(valid_words, 2):\n",
        "    vec_x, vec_y = model[x], model[y]\n",
        "    direction_xy = vec_y - vec_x\n",
        "    sim = np.dot(bias_direction, direction_xy) / (np.linalg.norm(bias_direction) * np.linalg.norm(direction_xy))\n",
        "    analogies.append((x, y, sim))\n",
        "\n",
        "analogies.sort(key=lambda tup: -tup[2])\n",
        "top_pairs = analogies[:15]\n",
        "print(\"top analogies aligned with israel → palestine:\\n\")\n",
        "for x, y, score in top_pairs:\n",
        "    print(f\"{x:12} → {y:12}  (similarity: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7dc403",
      "metadata": {
        "id": "6e7dc403"
      },
      "source": [
        "3. **Compare portrayals**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# good-bad portrayal axis\n",
        "\n",
        "good_words = ['good', 'peaceful', 'honest', 'hero', 'liberator']\n",
        "bad_words = ['bad', 'violent', 'corrupt', 'terrorist', 'oppressor']\n",
        "\n",
        "good_words = [w for w in good_words if w in model]\n",
        "bad_words = [w for w in bad_words if w in model]\n",
        "\n",
        "portrayal_axis = np.mean([model[bad] - model[good] for good, bad in zip(good_words, bad_words)], axis=0)"
      ],
      "metadata": {
        "id": "D3uFdkpZ_qjF"
      },
      "id": "D3uFdkpZ_qjF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing directional bias across each pair\n",
        "\n",
        "print(\"portrayal bias (positive = more aligned with 'bad'):\\n\")\n",
        "for x, y, _ in top_pairs:\n",
        "    proj_x = np.dot(model[x], portrayal_axis) / np.linalg.norm(portrayal_axis)\n",
        "    proj_y = np.dot(model[y], portrayal_axis) / np.linalg.norm(portrayal_axis)\n",
        "    print(f\"{x:12} score: {proj_x:+.4f}   |   {y:12} score: {proj_y:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m3kYpIoALwb",
        "outputId": "a935fbe3-af9b-4a01-d6ce-1c63f1debe21"
      },
      "id": "_m3kYpIoALwb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "portrayal bias (positive = more aligned with 'bad'):\n",
            "\n",
            "ceasefire    score: +0.0579   |   uprising     score: +0.3056\n",
            "occupation   score: +0.3438   |   uprising     score: +0.3056\n",
            "army         score: +0.0363   |   uprising     score: +0.3056\n",
            "military     score: +0.2891   |   uprising     score: +0.3056\n",
            "war          score: +0.1753   |   uprising     score: +0.3056\n",
            "defense      score: -0.0279   |   uprising     score: +0.3056\n",
            "strike       score: +0.1639   |   uprising     score: +0.3056\n",
            "peace        score: -0.3424   |   uprising     score: +0.3056\n",
            "government   score: +0.3242   |   uprising     score: +0.3056\n",
            "militant     score: +1.1886   |   uprising     score: +0.3056\n",
            "attack       score: +0.3147   |   uprising     score: +0.3056\n",
            "conflict     score: +0.2326   |   uprising     score: +0.3056\n",
            "nation       score: +0.0669   |   uprising     score: +0.3056\n",
            "army         score: +0.0363   |   leader       score: -0.3890\n",
            "freedom      score: -0.2288   |   uprising     score: +0.3056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of the Portrayals**\n",
        "\n",
        "I took the top analogy pairs that were most similar to the direction israel → palestine, and then projected both words in each pair onto a “good–bad” axis made from opposite pairs like peaceful → violent and hero → terrorist.\n",
        "\n",
        "The results show that the model tends to frame words like “uprising” more negatively than more neutral or state-aligned terms like “nation”.” It reflects a kind of bias in how conflict-related terms are represented, kind of like what would be expected from certain media coverage."
      ],
      "metadata": {
        "id": "wq6nLhqJAsja"
      },
      "id": "wq6nLhqJAsja"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
